{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to the database\n",
    "con = sqlite3.connect('amazon_data/database.sqlite')\n",
    "\n",
    "# filtering psitive and negative reviews\n",
    "filtered_data = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews WHERE Score != 3\n",
    "\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate positive and negative reviews \n",
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition)\n",
    "filtered_data['Score'] = positiveNegative\n",
    "# filtered_data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning: deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews WHERE Score != 3 AND UserId =\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sorting data according to productid in ascdending order\n",
    "sorted_data = filtered_data.sort_values('ProductId',axis=0,ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate entries \n",
    "final = sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what % of data still remains \n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data have helpfulnessdenominator less than helpfulnessnumerator\n",
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews WHERE Score != 3 AND Id=44737 OR Id=64422\n",
    "ORDER BY ProductID\n",
    "\"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing (Stemming, stopword removal and lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sentences containing HTML pages\n",
    "import re\n",
    "i =0 \n",
    "for sent in final['Text'].values:\n",
    "    if(len(re.findall('<.*?>',sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))#set of stopwords\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a snowball stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "sno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the word of any html tags\n",
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr,' ',sentence)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean word of any puctuations\n",
    "def cleanpunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)    \n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r'',cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for implementing step by step checks\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    for w in cleanhtml(sent).split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            print(cleaned_words)\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n",
    "                s = (sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                filtered_sentence.append(s)\n",
    "                if(final['Score'].values)[i] == 'positive':\n",
    "                    all_positive_words.append(s)\n",
    "                if(final['Score'].values)[i] == 'negative':\n",
    "                    all_negative_words.append(s)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    str1= b\" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText'] = final_string #adding column of cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the table into a sqlite database table for future\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c= conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews',conn,flavor-None,schema=None,if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bigrams ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most common frequent words:\",freq_dist_positive.most_common(20))\n",
    "print(\"Most common negative words:\",freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words like not should be avoided before building ngrams\n",
    "count_vect = CountVectorizer(ngram_range(1,2))\n",
    "final_bigram_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bigram_counts.get_shape() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(n_gram_range=(1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tf_idf.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_tf_idf[3,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tf_idf_feats(row,features,top_n=25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i],row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature','tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tf_idf_feats(final_tf_idf[1,:].toarray()[0],features,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
